{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "khPgQQVhkoT3"
   },
   "source": [
    "# Logistic regression with $\\ell_1$ regularization\n",
    "\n",
    "In this example, we use CVXPY to train a logistic regression classifier with $\\ell_1$ regularization. We are given data $(x_i,y_i)$, $i=1,\\ldots, m$. The $x_i \\in {\\bf R}^n$ are feature vectors, while the $y_i \\in \\{0, 1\\}$ are associated boolean classes; we assume the first component of each $x_i$ is $1$.\n",
    "\n",
    "Our goal is to construct a linear classifier $\\hat y = \\mathbb{1}[\\beta^T x > 0]$, which is $1$ when $\\beta^T x$ is positive and $0$ otherwise.  We model the posterior probabilities of the classes given the data linearly, with\n",
    "\n",
    "$$\n",
    "\\log \\frac{\\mathrm{Pr} (Y=1 \\mid X = x)}{\\mathrm{Pr} (Y=0 \\mid X = x)} = \\beta^T x.\n",
    "$$\n",
    "\n",
    "This implies that\n",
    "\n",
    "$$\n",
    "\\mathrm{Pr} (Y=1 \\mid X = x) = \\frac{\\exp(\\beta^T x)}{1 + \\exp(\\beta^T x)}, \\quad\n",
    "\\mathrm{Pr} (Y=0 \\mid X = x) = \\frac{1}{1 + \\exp(\\beta^T x)}.\n",
    "$$\n",
    "\n",
    "We fit $\\beta$ by maximizing the log-likelihood of the data, plus a regularization term $\\lambda \\|{\\beta_{1:}}\\|_1$ with $\\lambda > 0$:\n",
    "\n",
    "$$\n",
    "\\ell(\\beta) = \\sum_{i=1}^{m} y_i \\beta^T x_i - \\log(1 + \\exp (\\beta^T x_i)) - \\lambda \\|{\\beta_{1:}}\\|_1.\n",
    "$$\n",
    "\n",
    "Because $\\ell$ is a concave function of $\\beta$, this is a convex optimization problem.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "aPHSllwbTFHz"
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import cvxpy as cp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t1coFmORqxvb"
   },
   "source": [
    "In the following code we generate data with $n=20$ features by randomly choosing $x_i$ and a sparse $\\beta_{\\mathrm{true}} \\in {\\bf R}^n$.\n",
    "We then set $y_i = \\mathbb{1}[\\beta_{\\mathrm{true}}^T x_i  - z_i > 0]$, where the $z_i$ are i.i.d. normal random variables.\n",
    "We divide the data into training and test sets with $m=1000$ examples each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "PXGfqJFcTHmc"
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "n = 20\n",
    "m = 1000\n",
    "density = 0.2\n",
    "beta_true = np.random.randn(n)\n",
    "idxs = np.random.choice(range(n), int((1-density)*n), replace=False)\n",
    "for idx in idxs:\n",
    "    beta_true[idx] = 0\n",
    "\n",
    "sigma = 3\n",
    "X = np.random.normal(0, 5, size=(m,n))\n",
    "X[:, 0] = 1.0\n",
    "Y = X @ beta_true + np.random.normal(0, sigma, size=m)\n",
    "Y[Y > 0] = 1\n",
    "Y[Y <= 0] = 0\n",
    "\n",
    "X_test = np.random.normal(0, 5, size=(m, n))\n",
    "X_test[:, 0] = 1.0\n",
    "Y_test = X_test @ beta_true + np.random.normal(0, sigma, size=(m,1))\n",
    "Y_test[Y_test > 0] = 1\n",
    "Y_test[Y_test <= 0] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q6MHXEnvrrIM"
   },
   "source": [
    "We next formulate the optimization problem using CVXPY."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": false,
    "id": "uqtlF0c6TywI"
   },
   "outputs": [],
   "source": [
    "beta = cp.Variable(n)\n",
    "lambd = cp.Parameter(nonneg=True)\n",
    "log_likelihood = cp.sum(\n",
    "    cp.multiply(Y, X @ beta) -\n",
    "    cp.log_sum_exp(cp.vstack([np.zeros(m), X @ beta]), axis=0) - \n",
    "    lambd * cp.norm(beta[1:], 1)\n",
    ")\n",
    "problem = cp.Problem(cp.Maximize(log_likelihood))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I20ls_5Arw2e"
   },
   "source": [
    "We solve the optimization problem for a range of $\\lambda$ to compute a trade-off curve.\n",
    "We then plot the train and test error over the trade-off curve.\n",
    "A reasonable choice of $\\lambda$ is the value that minimizes the test error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "UtTjJdbggRPw"
   },
   "outputs": [],
   "source": [
    "def error(scores, labels):\n",
    "  scores[scores > 0] = 1\n",
    "  scores[scores <= 0] = 0\n",
    "  return np.sum(np.abs(scores - labels)) / float(np.size(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "rM7RYHuGbQ-q"
   },
   "outputs": [],
   "source": [
    "trials = 100\n",
    "train_error = np.zeros(trials)\n",
    "test_error = np.zeros(trials)\n",
    "lambda_vals = np.logspace(-2, 0, trials)\n",
    "beta_vals = []\n",
    "for i in range(trials):\n",
    "    lambd.value = lambda_vals[i]\n",
    "    problem.solve()\n",
    "    train_error[i] = error(X @ beta.value, Y)\n",
    "    test_error[i] = error(X_test @ beta.value, Y_test)\n",
    "    beta_vals.append(beta.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 383
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "oi2Ucb0bhN1y",
    "outputId": "2d36ca03-d327-45e8-e2fe-7841d1ecd702"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "\n",
    "plt.plot(lambda_vals, train_error, label=\"Train error\")\n",
    "plt.plot(lambda_vals, test_error, label=\"Test error\")\n",
    "plt.xscale('log')\n",
    "plt.legend(loc='upper left')\n",
    "plt.xlabel(r\"$\\lambda$\", fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W9IlUQbfr774"
   },
   "source": [
    "We also plot the regularization path, or the $\\beta_i$ versus $\\lambda$. Notice that \n",
    "a few features remain non-zero longer for larger $\\lambda$ than the rest, which suggests that these features are the most important. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 383
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "exHZkiZ9kIBw",
    "outputId": "64eedbe2-6b07-4061-bf40-7cd5deb7dd6a"
   },
   "outputs": [],
   "source": [
    "for i in range(n):\n",
    "    plt.plot(lambda_vals, [wi for wi in beta_vals])\n",
    "plt.xlabel(r\"$\\lambda$\", fontsize=16)\n",
    "plt.xscale(\"log\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "cvxpy_logistic_regression.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
